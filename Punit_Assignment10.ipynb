{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkTaafg6ixmM",
        "outputId": "6c5ef13d-302f-40f0-9c67-7617694a81d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn nltk\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Check the dataset\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "print(f\"Unique labels: {np.unique(y_train)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7fcOZN8jFdZ",
        "outputId": "3ccbc62d-2a30-4ec5-f827-47c22d753e09"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 25000\n",
            "Testing samples: 25000\n",
            "Unique labels: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the word index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Create a reverse word index\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "# Decoding a sample\n",
        "def decode_review(encoded_review):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])  # Adjust for off-by-three\n",
        "\n",
        "# Example\n",
        "print(decode_review(X_train[0]))  # Decoded review from the first training example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjk5-UzWjNkH",
        "outputId": "edb3d1aa-6fa9-4268-d375-0d712525f25e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_processed = [preprocess_text(decode_review(review)) for review in X_train]\n",
        "X_test_processed = [preprocess_text(decode_review(review)) for review in X_test]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1pIu7dXjTSG",
        "outputId": "26b15f0b-9495-4377-b43b-2e72d0f31c47"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # using unigram and bigram\n",
        "\n",
        "# Fit and transform training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_processed)\n",
        "\n",
        "# Transform test data\n",
        "X_test_tfidf = vectorizer.transform(X_test_processed)\n",
        "\n",
        "# Shape of feature matrices\n",
        "print(f\"Train set shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Test set shape: {X_test_tfidf.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcMXrr8rjYu-",
        "outputId": "c2e1ff22-5da5-4f1a-cf61-b3004f2ae54b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (25000, 5000)\n",
            "Test set shape: (25000, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Model selection\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'SVM': SVC()\n",
        "}\n",
        "\n",
        "# Train models and evaluate\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    print(f\"Accuracy for {model_name}: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9S-jHFCjdhR",
        "outputId": "fe2165c6-75fd-42f4-d1da-e6eb1bfe115f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression...\n",
            "Accuracy for Logistic Regression: 0.88496\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.88     12500\n",
            "           1       0.88      0.89      0.89     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "[[11018  1482]\n",
            " [ 1394 11106]]\n",
            "\n",
            "Training Naive Bayes...\n",
            "Accuracy for Naive Bayes: 0.85276\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85     12500\n",
            "           1       0.85      0.85      0.85     12500\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.85      0.85      0.85     25000\n",
            "weighted avg       0.85      0.85      0.85     25000\n",
            "\n",
            "[[10674  1826]\n",
            " [ 1855 10645]]\n",
            "\n",
            "Training SVM...\n",
            "Accuracy for SVM: 0.88492\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.88     12500\n",
            "           1       0.88      0.89      0.89     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "[[11034  1466]\n",
            " [ 1411 11089]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
        "    ('classifier', LogisticRegression())  # Replace with any model you want\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'vectorizer__max_features': [5000, 10000],\n",
        "    'classifier__C': [0.1, 1, 10]  # Regularization for Logistic Regression\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1)\n",
        "grid_search.fit(X_train_processed, y_train)\n",
        "\n",
        "# Best model and parameters\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Score: {grid_search.best_score_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDz7G6dIjjMw",
        "outputId": "f2042bc6-0e23-4245-ef23-c5730206f4b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__C': 1, 'vectorizer__max_features': 10000}\n",
            "Best Score: 0.8871599596405263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "custom_reviews = [\n",
        "    \"I love this movie, it's amazing!\",\n",
        "    \"This film was a waste of time.\"\n",
        "]\n",
        "\n",
        "# Preprocess the review\n",
        "custom_reviews_processed = [preprocess_text(review) for review in custom_reviews]\n",
        "\n",
        "\n",
        "#  Use the best model\n",
        "model = grid_search.best_estimator_\n",
        "\n",
        "#  Predict sentiment for the custom reviews\n",
        "\n",
        "predictions = model.predict(custom_reviews_processed)\n",
        "\n",
        "# Display the result\n",
        "for review, pred in zip(custom_reviews, predictions):\n",
        "    sentiment = 'Positive' if pred == 1 else 'Negative'\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Prediction: {sentiment}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsT23jRw9ymz",
        "outputId": "d6682fe9-932d-45f7-8284-cb68255d9ade"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: I love this movie, it's amazing!\n",
            "Prediction: Positive\n",
            "\n",
            "Review: This film was a waste of time.\n",
            "Prediction: Negative\n",
            "\n"
          ]
        }
      ]
    }
  ]
}